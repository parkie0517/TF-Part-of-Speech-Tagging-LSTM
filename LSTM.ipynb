{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ikFHxHEsaCH_",
        "YBal4Kwb18_B",
        "LoDND_2J3G_v"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 환경설정"
      ],
      "metadata": {
        "id": "ikFHxHEsaCH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "EarF2GxMaByG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XtH7cuGadLW",
        "outputId": "ae2fab0e-7e3d-4b5e-a207-e216a137bc97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Classroom/23-2_AI_Class/Assignment4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYd0DCC7sGkt",
        "outputId": "e49669ea-7dd1-4b98-9583-f9c8c54ba19f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Classroom/23-2_AI_Class/Assignment4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceHRrpoDsJHo",
        "outputId": "e4142482-3c41-433d-e627-51ce8facfa3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_index_sentences_test.txt        all_word_pos_sentences_test.txt\n",
            "all_index_sentences_train.txt       all_word_pos_sentences_train.txt\n",
            "all_index_sentences_validation.txt  all_word_pos_sentences_validation.txt\n",
            "all_word_pos_sentences_all.txt      assignment4.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 데이터준비"
      ],
      "metadata": {
        "id": "PYqcmTsaaINv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary_temp(corpus_path):\n",
        "    \"\"\"\n",
        "    단어와 그 출현횟수를 dict 타입의 사전에 수집한다.\n",
        "    \"\"\"\n",
        "    Vocab_temporary = {}\n",
        "    fp = open(corpus_path, \"r\", encoding=\"utf-8\")\n",
        "    for line in fp.readlines():\n",
        "        sentence = line.split()\n",
        "        if sentence[0] == '<<':\n",
        "            continue\n",
        "        for word_pos_pair in sentence:\n",
        "            w_p = word_pos_pair.split('/')\n",
        "            nseg = len(w_p)\n",
        "            ## word/pos 내에 슬래시가 2개 이상 있어 3 조각 이상이 나옴.\n",
        "            if nseg > 2:    ## 마지막 슬래시를 기준으로 단어와 품사로 구분함.\n",
        "                word = ''\n",
        "                for i in range(nseg-1):\n",
        "                    word = word + w_p[i] + '/'\n",
        "                word = word[:-1]    # remove the last slash.\n",
        "            else:\n",
        "                word = w_p[0]\n",
        "\n",
        "            pos = w_p[-1]   # the last segment\n",
        "            if word in Vocab_temporary:\n",
        "                Vocab_temporary[word] += 1 # value값을 1 증가시키기\n",
        "            else:\n",
        "                Vocab_temporary[word] = 1\n",
        "    fp.close()\n",
        "    return Vocab_temporary\n",
        "\n",
        "# 1 차적인 임시사전을 만든다. 사전명: Vocab_temporary\n",
        "# key: 단어, value:출현횟수\n",
        "Vocab_temporary = build_vocabulary_temp(\"./all_word_pos_sentences_all.txt\")"
      ],
      "metadata": {
        "id": "SafeHUY2tIFz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 단어들을 출현횟수로 내림차순으로 정렬하여 그 결과를 리스트로 받는다.\n",
        "sorted_Vocab = sorted(Vocab_temporary.items(), key = lambda kv: kv[1], reverse=True)\n",
        "Total_n_words = len(sorted_Vocab)\n",
        "print('파일에서 모은 총 단어수:', Total_n_words)\n",
        "print(f'sorted_Vocab의 일부 내용 출력: {sorted_Vocab[0:10]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TMlEr98xAGs",
        "outputId": "deb7b5b8-b6fa-41f6-b13c-17bc39f877ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파일에서 모은 총 단어수: 51457\n",
            "sorted_Vocab의 일부 내용 출력: [(',', 66321), ('the', 56317), ('.', 52726), ('of', 31003), ('to', 29976), ('a', 25848), ('and', 21632), ('in', 20765), (\"'s\", 12661), ('that', 10921)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 개의 특수단어('[PAD]', '[UNK]')를 포함하는 정식 사전(사전명: Vocab) 을 만든다:\n",
        "# key: 단어, value: 단어번호\n",
        "Vocab = {}\n",
        "Vocab['[PAD]'] = 0  # Padding을 의미하는 특수단어 추가\n",
        "Vocab['[UNK]'] = 1  # Unkown을 의미하는 특수단어 추가\n",
        "\n",
        "# Vocab_temporary 에 모은 단어들에게는 단어번호를 2 부터 준다.\n",
        "for i in range(Total_n_words):\n",
        "    word = sorted_Vocab[i][0]\n",
        "    freq = sorted_Vocab[i][1]\n",
        "    Vocab[word] = i + 2\n",
        "\n",
        "Total_number_words = len(Vocab)\n",
        "print(\"최종 사전의 총 단어수:\", Total_number_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN5VBnsqxSyW",
        "outputId": "1d891d7a-b813-4ee6-c2a7-3f5f7185f1fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종 사전의 총 단어수: 51459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 역-단어사전 i_Vocab  만들기:  Vocab 에서 key와 value 를 바꾼 사전.\n",
        "# key: 단어번호, value: 단어\n",
        "all_wps = list(Vocab.keys())\n",
        "i_Vocab = {}\n",
        "for word in all_wps:\n",
        "  widx = Vocab[word]\n",
        "  i_Vocab[widx] = word"
      ],
      "metadata": {
        "id": "Ni0_O4sAxydY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 품사 사전 (사전명: dic_POS)만들기:  key:품사명,  value: 품사번호\n",
        "# 두 특수단어에게는 품사명/품사번호를  '[PAD]': 0,     '[UNK]': 1 로 준다.\n",
        "# 나머지는 penn-tree-bank 의 48 개의 품사들에게 품사 번호를 2 부터 부여한다(2~49)\n",
        "# 결국 총 품사는 총 50 개로 번호는 0 ~ 49 가 된다.\n",
        "all_pos_list = ['[PAD]', '[UNK]',\n",
        "                'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS',\n",
        "                'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB',\n",
        "                'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN',\n",
        "                'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '#', '$', '.', ',',\n",
        "                ':', '(', ')', '\\'\\'', '\\'', '``', '&rsquo', '”']\n",
        "dic_POS = {}\n",
        "for i in range(len(all_pos_list)):\n",
        "    dic_POS[all_pos_list[i]] = i\n",
        "\n",
        "num_pos = len(dic_POS)\n",
        "print(\"총 품사수: \", num_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB-bbraaySbP",
        "outputId": "bc14f5bc-77d4-442f-b94c-0644cfe6f41c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 품사수:  50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 역-품사사전 만들기: dic_POS 에서 key와 value 를 바꾼 사전.\n",
        "# key: 품사번호, value: 품사명\n",
        "all_pos = list(dic_POS.keys())\n",
        "i_dic_POS = {}\n",
        "for a_pos in all_pos:\n",
        "  pidx = dic_POS[a_pos]\n",
        "  i_dic_POS[pidx] = a_pos"
      ],
      "metadata": {
        "id": "93Fef4gbya5i"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어/품사명으로된 파일로부터 단어번호/품사번호으로된 파일을 생성한다\n",
        "def build_index_sentences(path_word_pos_sentence_file, path_index_sentence_file):\n",
        "    fp = open(path_word_pos_sentence_file, \"r\", encoding=\"utf-8\")\n",
        "    fp_w = open(path_index_sentence_file, \"w\", encoding=\"utf-8\")\n",
        "\n",
        "    for line in fp.readlines():\n",
        "        sentence = line.split()\n",
        "        if sentence[0] == '<<':   ## 파일명을 가지는 줄은 무시한다.\n",
        "            continue\n",
        "\n",
        "        line_widx = ''  # line for word indices\n",
        "        line_pidx = ''  # line for pos indices\n",
        "\n",
        "        for word_pos_pair in sentence:\n",
        "            w_p = word_pos_pair.split('/')\n",
        "            nseg = len(w_p)\n",
        "            if nseg > 2:\n",
        "                word = ''\n",
        "                for i in range(nseg-1):\n",
        "                    word = word + w_p[i] + '/'\n",
        "                word = word[:-1]    # remove the last slash.\n",
        "            else:\n",
        "                word = w_p[0]\n",
        "\n",
        "            pos = w_p[-1]   # the last segment\n",
        "            if not(word in Vocab):  # Other scheme: Vocab.get(word) 가 None 이면 없는 것을 말함.\n",
        "                widx = 1    # give index of [UNK] since it is missing in Vocab.\n",
        "            else:\n",
        "                widx = Vocab[word]\n",
        "\n",
        "            if not(pos in dic_POS):\n",
        "                pos_list = pos.split('|')\n",
        "                pos = pos_list[-1]\n",
        "                if not (pos in dic_POS):\n",
        "                    print(\"exception occurs at dic_POS look_up. w_p=\", w_p, \" pos=\", pos)\n",
        "                    time.sleep(100)\n",
        "                else:\n",
        "                    pidx = dic_POS[pos]\n",
        "            else:\n",
        "                pidx = dic_POS[pos]\n",
        "\n",
        "            if len(line_widx) == 0:\n",
        "                line_widx = line_widx + str(widx)\n",
        "            else:\n",
        "                line_widx = line_widx + '\\t' + str(widx)\n",
        "\n",
        "            if len(line_pidx) == 0:\n",
        "                line_pidx = line_pidx + str(pidx)\n",
        "            else:\n",
        "                line_pidx = line_pidx + '\\t' + str(pidx)\n",
        "\n",
        "        fp_w.write(line_widx + '\\n')\n",
        "        fp_w.write(line_pidx + '\\n')\n",
        "        fp_w.write('\\n')    # an empty line after each sentence\n",
        "    fp_w.close()\n",
        "    fp.close()\n",
        "\n",
        "build_index_sentences(\"./all_word_pos_sentences_train.txt\", \"./all_index_sentences_train.txt\")\n",
        "build_index_sentences(\"./all_word_pos_sentences_validation.txt\", \"./all_index_sentences_validation.txt\")\n",
        "build_index_sentences(\"./all_word_pos_sentences_test.txt\", \"./all_index_sentences_test.txt\")"
      ],
      "metadata": {
        "id": "1wFVH-ozz5oh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련예제 준비 함수\n",
        "# 파일경로를 입력으로 받는다. 이 파일은 all_index_sentences_???.txt 이다.\n",
        "# 출력: 파일내의 모든 문장들에 대한 정보를 이용하여 다음을 준비하여 출력한다.\n",
        "#   1) list_X: 문장들의 단어 번호 리스트를 원소로 가지는 리스트\n",
        "#   2) list_Y: 문장들의 정답품사번호 리스트를 원소로 가지는 리스트\n",
        "#   3) list_leng: 문장들의 길이를 원소로 가지는 리스트\n",
        "\n",
        "# Hyperparamter 정의\n",
        "Max_seq_length = 128    ## time step 수 (MSL)\n",
        "\n",
        "def load_X_and_Y(path_index_file):\n",
        "    fp= open(path_index_file, \"r\", encoding=\"utf-8\")\n",
        "    list_X = []\n",
        "    list_Y = []\n",
        "    list_leng = []\n",
        "\n",
        "    while True:\n",
        "        # read two lines\n",
        "        wordline = fp.readline()\n",
        "        line_leng = len(wordline)\n",
        "\n",
        "        if line_leng == 0:\n",
        "            break   # end of file has come.\n",
        "        if line_leng == 1:\n",
        "            continue    # empty line used as sentence delimeter\n",
        "\n",
        "        # The line read just before is a line of word indices.\n",
        "        # The next line should be the corresponding pos index line.\n",
        "        posline = fp.readline()\n",
        "        w_index = wordline.split()\n",
        "        p_index = posline.split()\n",
        "\n",
        "        # X : a list of indices of words in a sentence.\n",
        "        # Y : a list of pos indices of words in the sentence of X.\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "        leng = len(w_index)\n",
        "        if leng > Max_seq_length:\n",
        "            leng = Max_seq_length   # truncation is done.\n",
        "\n",
        "        for i in range(leng):\n",
        "            X.append(int(w_index[i]))\n",
        "            Y.append(int(p_index[i]))\n",
        "\n",
        "        # pads are added after sentence\n",
        "        if leng < Max_seq_length:\n",
        "            for i in range(leng, Max_seq_length):\n",
        "                X.append(0)     # word index of '[PAD]' which is 0 is added.\n",
        "                Y.append(0)     # pos index of  '[PAD]' which is 0 is added.\n",
        "\n",
        "        list_X.append(X)\n",
        "        list_Y.append(Y)\n",
        "        list_leng.append(leng)\n",
        "\n",
        "    fp.close()\n",
        "    return list_X, list_Y, list_leng\n",
        "\n",
        "# Training 데이터\n",
        "x_train, y_train, leng_train = load_X_and_Y(\"./all_index_sentences_train.txt\")\n",
        "x_train = np.array(x_train, dtype='i')\n",
        "y_train = np.array(y_train, dtype='i')\n",
        "\n",
        "# Validation 데이터\n",
        "x_validation, y_validation, leng_valiation = load_X_and_Y(\"./all_index_sentences_validation.txt\")\n",
        "x_validation = np.array(x_validation, dtype='i')\n",
        "y_validation = np.array(y_validation, dtype='i')\n",
        "\n",
        "# Test 데이터\n",
        "x_test, y_test, leng_test = load_X_and_Y(\"./all_index_sentences_test.txt\")\n",
        "x_test = np.array(x_test, dtype='i')\n",
        "y_test = np.array(y_test, dtype='i')"
      ],
      "metadata": {
        "id": "8Vf3Q53A1iNH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 모델 정의"
      ],
      "metadata": {
        "id": "YBal4Kwb18_B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yU_LfUarZowd"
      },
      "outputs": [],
      "source": [
        "# 상수 선언\n",
        "Vocab_size = 51459\t# 사전 Vocab 의 총 단어수\n",
        "Num_POS = 50    ## 총 품사 수\n",
        "\n",
        "# Hyperparamter 정의\n",
        "d_embed = 100     ## word embedding vector 의 길이 (원소수)\n",
        "\n",
        "# Model 설계\n",
        "model = tf.keras.models.Sequential()\n",
        "# 층0:  word-embedding 층\n",
        "model.add(tf.keras.layers.Embedding(Vocab_size, d_embed, embeddings_initializer='random_normal', input_length=Max_seq_length, mask_zero=True, trainable=True))\t# output shape: (bsz, MSL, d_emb)\n",
        "# 충1: LSTM 층\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), input_shape=(Max_seq_length, d_embed)))\n",
        "# output shape: (batch_sz, MSL, 512)\n",
        "# 충2: LSTM 층\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True), input_shape=(Max_seq_length, 512)))\n",
        "# output shape: (batch_sz, MSL, 256)\n",
        "# 충3: LSTM 층\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True), input_shape=(Max_seq_length, 256)))\n",
        "# output shape: (batch_sz, MSL, 128)\n",
        "# 층4: NN 층\n",
        "model.add(tf.keras.layers.Dense(units=Num_POS, activation='softmax', use_bias=True))\t# 최종출력층. 각 시간의 각 단어마다 num_POS=50개의 확률이 생성됨.\n",
        "# output shape: (batch_sz, MSL, num_POS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 학습"
      ],
      "metadata": {
        "id": "LoDND_2J3G_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparamter 정의\n",
        "LEARNING_RATE = 0.7e-4\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
        "model.fit(x=x_train, y=y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_validation, y_validation), shuffle=True, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2CdR7sY2K6u",
        "outputId": "adb56fe7-80a0-4312-edc7-f46e7045c713"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "844/844 [==============================] - 117s 102ms/step - loss: 2.9236 - acc: 0.1831 - val_loss: 2.7986 - val_acc: 0.1974\n",
            "Epoch 2/20\n",
            "844/844 [==============================] - 46s 55ms/step - loss: 2.7475 - acc: 0.2049 - val_loss: 2.7203 - val_acc: 0.2078\n",
            "Epoch 3/20\n",
            "844/844 [==============================] - 42s 50ms/step - loss: 2.4823 - acc: 0.3114 - val_loss: 2.0908 - val_acc: 0.4303\n",
            "Epoch 4/20\n",
            "844/844 [==============================] - 43s 51ms/step - loss: 1.6605 - acc: 0.5383 - val_loss: 1.3132 - val_acc: 0.6416\n",
            "Epoch 5/20\n",
            "844/844 [==============================] - 38s 44ms/step - loss: 1.0161 - acc: 0.7456 - val_loss: 0.8375 - val_acc: 0.8010\n",
            "Epoch 6/20\n",
            "844/844 [==============================] - 42s 49ms/step - loss: 0.6464 - acc: 0.8581 - val_loss: 0.5862 - val_acc: 0.8707\n",
            "Epoch 7/20\n",
            "844/844 [==============================] - 42s 50ms/step - loss: 0.4445 - acc: 0.9042 - val_loss: 0.4482 - val_acc: 0.8972\n",
            "Epoch 8/20\n",
            "844/844 [==============================] - 39s 47ms/step - loss: 0.3274 - acc: 0.9276 - val_loss: 0.3701 - val_acc: 0.9129\n",
            "Epoch 9/20\n",
            "844/844 [==============================] - 41s 48ms/step - loss: 0.2567 - acc: 0.9393 - val_loss: 0.3201 - val_acc: 0.9221\n",
            "Epoch 10/20\n",
            "844/844 [==============================] - 43s 52ms/step - loss: 0.2096 - acc: 0.9498 - val_loss: 0.2891 - val_acc: 0.9307\n",
            "Epoch 11/20\n",
            "844/844 [==============================] - 39s 47ms/step - loss: 0.1763 - acc: 0.9574 - val_loss: 0.2660 - val_acc: 0.9349\n",
            "Epoch 12/20\n",
            "844/844 [==============================] - 41s 49ms/step - loss: 0.1521 - acc: 0.9617 - val_loss: 0.2515 - val_acc: 0.9381\n",
            "Epoch 13/20\n",
            "844/844 [==============================] - 40s 47ms/step - loss: 0.1349 - acc: 0.9649 - val_loss: 0.2387 - val_acc: 0.9408\n",
            "Epoch 14/20\n",
            "844/844 [==============================] - 40s 48ms/step - loss: 0.1216 - acc: 0.9674 - val_loss: 0.2292 - val_acc: 0.9424\n",
            "Epoch 15/20\n",
            "844/844 [==============================] - 39s 47ms/step - loss: 0.1114 - acc: 0.9695 - val_loss: 0.2239 - val_acc: 0.9441\n",
            "Epoch 16/20\n",
            "844/844 [==============================] - 36s 43ms/step - loss: 0.1030 - acc: 0.9711 - val_loss: 0.2218 - val_acc: 0.9443\n",
            "Epoch 17/20\n",
            "844/844 [==============================] - 41s 48ms/step - loss: 0.0965 - acc: 0.9725 - val_loss: 0.2212 - val_acc: 0.9446\n",
            "Epoch 18/20\n",
            "844/844 [==============================] - 39s 46ms/step - loss: 0.0911 - acc: 0.9735 - val_loss: 0.2177 - val_acc: 0.9460\n",
            "Epoch 19/20\n",
            "844/844 [==============================] - 40s 48ms/step - loss: 0.0867 - acc: 0.9744 - val_loss: 0.2157 - val_acc: 0.9460\n",
            "Epoch 20/20\n",
            "844/844 [==============================] - 41s 48ms/step - loss: 0.0828 - acc: 0.9753 - val_loss: 0.2170 - val_acc: 0.9459\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7914e447a9e0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 추론"
      ],
      "metadata": {
        "id": "XMoRMfgZ3te_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(x=x_test, verbose=1)   # batch 단위로  수행함 (default = 32)    # batch 별 결과를 모아서 전체 결과를 반환함\n",
        "pred_label = tf.math.argmax(pred, axis=2)\n",
        "pred_label = pred_label.numpy()     # this is of 2-dimension (num_test_sents, msl)\n",
        "num_test_sentences = y_test.shape[0]  # y_test has a shape of (num_test_sents, msl)"
      ],
      "metadata": {
        "id": "VwsCKGtb3NR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0140bd6e-21bd-498b-9c86-e1e2a0fe368b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240/240 [==============================] - 12s 18ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 결과 분석"
      ],
      "metadata": {
        "id": "zyXmX9BRAUcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct = 0\n",
        "\n",
        "for i in range(len(leng_test)):\n",
        "    for j in range(leng_test[i]):\n",
        "        if y_test[i][j] == pred_label[i][j]:\n",
        "            num_correct += 1\n",
        "\n",
        "acc = num_correct/sum(leng_test)*100"
      ],
      "metadata": {
        "id": "TFMRLnL1mkCv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test accuracy = \", acc)\n",
        "print(\"Program ends.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbVXU0c6zNeR",
        "outputId": "da7a4680-5632-4af3-9a63-d30bb9630c69"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy =  94.88637145728089\n",
            "Program ends.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "    print(f'({i+1}) ', end=\"\")\n",
        "    for j in range(leng_test[i]):\n",
        "        print(i_Vocab[x_test[i][j]]+ \"/\"+i_dic_POS[pred_label[i][j]],end=\"\")\n",
        "        if y_test[i][j] != pred_label[i][j]:\n",
        "            print(\"<\"+ i_dic_POS[y_test[i][j]] + \">\",end=\"\")\n",
        "        print(\" \", end=\"\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "d50NkUsiAVmh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e3f029-882d-4424-bf04-b77e954d95fd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1) Arthur/NNP M./NNP Goldberg/NNP said/VBD he/PRP extended/VBD his/PRP$ unsolicited/JJ tender/NN offer/NN of/IN $/$ 32/CD \n",
            "(2) a/DT share/NN tender/NN offer/NN ,/, or/CC $/$ 154.3/NN<CD> million/CD ,/, for/IN Di/NNP Giorgio/NNP Corp./NNP to/TO Nov./NNP 1/CD ./. \n",
            "(3) DIG/NNP Acquisition/NNP Corp./NNP ,/, the/DT New/NNP Jersey/NNP investor/NN 's/POS acquisition/NN vehicle/NN ,/, said/VBD that/IN as/IN of/IN the/DT close/NN of/IN business/NN yesterday/NN ,/, 560,839/VBG<CD> shares/NNS had/VBD been/VBN tendered/VBN ./. \n",
            "(4) Including/VBG the/DT stake/NN DIG/NNP already/RB held/VBN<VBD> ,/, DIG/NNP holds/VBZ a/DT total/NN of/IN about/RB 25/CD %/NN of/IN Di/NNP Giorgio/NNP 's/POS shares/NNS on/IN a/DT fully/RB diluted/VBN basis/NN ./. \n",
            "(5) The/DT offer/NN ,/, which/WDT also/RB includes/VBZ common/JJ and/CC preferred/JJ<VBN> stock/NN purchase/NN rights/NNS ,/, was/VBD to/TO expire/VB last/JJ night/NN at/IN midnight/NN ./. \n",
            "(6) The/DT new/JJ expiration/NN date/NN is/VBZ the/DT date/NN on/IN which/WDT \n",
            "(7) DIG/NNP 's/POS financing/NN<VBG> commitments/NNS ,/, which/WDT total/VBP about/IN<RB> $/$ 240/CD million/CD ,/, are/VBP to/TO expire/VB ./. \n",
            "(8) DIG/NNP is/VBZ a/DT unit/NN of/IN DIG/NNP Holding/NNP Corp./NNP ,/, a/DT unit/NN of/IN Rose/NNP Partners/NNP L.P/NNP ./. \n",
            "(9) Mr./NNP Goldberg/NNP is/VBZ the/DT sole/JJ general/JJ partner/NN in/IN Rose/NNP Partners/NNP ./. \n",
            "(10) In/IN August/NNP ,/, Di/NNP Giorgio/NNP ,/, a/DT San/NNP Francisco/NNP food/NN products/NNS and/CC building/NN materials/NNS marketing/NN and/CC distribution/NN company/NN ,/, rejected/VBD Mr./NNP Goldberg/NNP 's/POS offer/NN as/IN inadequate/JJ ./. \n",
            "(11) In/IN New/NNP York/NNP Stock/NNP Exchange/NNP composite/JJ<NN> trading/NN yesterday/NN ,/, Di/NNP Giorgio/NNP closed/VBD at/IN $/$ 31.50/CD \n",
            "(12) a/DT share/NN ,/, down/RB $/$ 1.75/CD ./. \n",
            "(13) What/WP does/VBZ n't/RB belong/VB here/RB ?/. \n",
            "(14) A./NNP<LS> manual/NNP<JJ> typewriters/NNS ,/, \n",
            "(15) B./NNP<LS> black-and-white/JJ snapshots/NNS ,/, \n",
            "(16) C./NNP<LS> radio/NN adventure/NN shows/VBZ<NNS> ./. \n",
            "(17) If/IN you/PRP guessed/VBD black-and-white/JJ snapshots/NNS ,/, you/PRP 're/VBP right/JJ ./. \n",
            "(18) After/IN years/NNS of/IN fading/JJ<NN> into/IN the/DT background/NN ,/, two-tone/JJ photography/NN is/VBZ coming/VBG back/RB ./. \n",
            "(19) Trendy/JJ magazine/NN advertisements/NNS feature/VBP stark/JJ black-and-white/JJ photos/NNS of/IN Hollywood/NNP celebrities/NNS pitching/VBG jeans/NNS ,/, shoes/NNS and/CC liquor/NN ./. \n",
            "(20) Portrait/CD<NN> studios/NNS accustomed/VBN to/TO shooting/VBG only/RB in/IN color/NN report/NN<VBP> a/DT rush/NN to/TO black-and-white/JJ portrait/NN orders/NNS ./. \n"
          ]
        }
      ]
    }
  ]
}